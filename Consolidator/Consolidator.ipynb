{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidator Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements - some need to be installed separately from Anaconda base installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Successful\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#import snowflake.connector\n",
    "#pip install --upgrade snowflake-connector-python <- use this to install the snowflake connector package\n",
    "#conda install -c conda-forge snowflake-connector-python\n",
    "import pandas as pd\n",
    "import csv\n",
    "import fastparquet\n",
    "from fastparquet import *\n",
    "#Conda install fastparquet\n",
    "import glob\n",
    "import pyodbc\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import datetime\n",
    "import requests\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import pickle\n",
    "print(\"Import Successful\")\n",
    "\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replacy(a,b):\n",
    "    while a in b:\n",
    "        b = b.replace(a,'|')\n",
    "    return b\n",
    "\n",
    "\n",
    "\n",
    "def Splitty(x,y):\n",
    "    SL = ['\\\\','.']\n",
    "    for item in SL:\n",
    "        x = Replacy(item,x)\n",
    "        x = x.split('|')\n",
    "    return x[y]\n",
    "\n",
    "def GetTableName(f):\n",
    "    fpath = str(f).split(\".\")[0]\n",
    "    tablename = str(fpath.split(\"\\\\\")[-2])\n",
    "    return tablename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful baseline Load Functions for different File Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_CSV(f):\n",
    "    with open(f, 'r') as r:\n",
    "        S = [item.split(\",\")for item in r.readlines()]\n",
    "    return S\n",
    "\n",
    "def Load_SQL_DDL(f):\n",
    "    with open(f, 'r') as r:\n",
    "        S = r.readlines()\n",
    "    return S\n",
    "\n",
    "def Load_JSON(f):\n",
    "    with open(f, 'r') as r:\n",
    "        jmain = json.load(r)\n",
    "    return jmain\n",
    "\n",
    "def ParseXML(f,s):\n",
    "    tree = ET.parse(f)\n",
    "    root = tree.getroot()\n",
    "    for child in root.iter('{http://www.mediawiki.org/xml/export-0.10/}text'):\n",
    "        parse = child.text\n",
    "    return parse.split(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "def Get_Locations(f,start,end):\n",
    "    #D = Load_CSV('Data\\Site_Data.csv')\n",
    "    D = Load_CSV(f)\n",
    "    DL = [[line[-2],line[-1].replace('\\n',''),line[0]] for line in D if line[5] == 'PJM' and line[-1] != \"\" and line[-2] != \"\"]\n",
    "    DSList = []\n",
    "    for line in DL:\n",
    "        DS = Get_Location_Data(line[0],line[1],start,end)\n",
    "        DSList.append([line[2],DS])\n",
    "    return DSList\n",
    "\n",
    "def Get_Location_Data(lat,long,start,end):\n",
    "    data = '{ \"lat\": '+str(lat)+', \"lon\": '+str(long)+',\"startDate\": \"'+str(start)+'\",\"endDate\": \"'+str(end)+'\" }'\n",
    "    response = requests.post('http://weather.tceh.net/weather_historical_hourly/', headers=headers, data=data)\n",
    "    r = response.json()\n",
    "    return r\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def Get_Station(lat,long,start,end):\n",
    "    \n",
    "    \n",
    "    data = '{ \"lat\": '+str(lat)+', \"lon\": '+str(long)+',\"startDate\": \"'+str(start)+'\",\"endDate\": \"'+str(end)+'\" }'\n",
    "    response = requests.post('http://weather.tceh.net/weather_historical_hourly/', headers=headers, data=data)\n",
    "    r = response.json()\n",
    "    s = [r[0]['Latitude'],r[0]['Longitude']]\n",
    "    return s\n",
    "\n",
    "def Flatten_Data(DSList):\n",
    "    klist = []\n",
    "    klist.append('VistraSite')\n",
    "    for d in DSList:\n",
    "        for entry in d[1]:\n",
    "            for k in entry:\n",
    "                if k not in klist:\n",
    "                    klist.append(k)\n",
    "    flist = []\n",
    "    #with open('WeatherDataForFred.csv', 'w') as export:\n",
    "        #export.write(klist)\n",
    "    flist.append(klist)\n",
    "    for d in DSList:\n",
    "        for entry in d[1]:\n",
    "            alist = []\n",
    "            alist.append(d[0])\n",
    "            for k in klist[1:]:\n",
    "                try:\n",
    "                    alist.append(entry[k])\n",
    "                except:\n",
    "                    alist.append('NA')\n",
    "            #export.write(alist)\n",
    "            flist.append(alist)\n",
    "    return flist\n",
    "\n",
    "\n",
    "#Flatten_Data(Get_Data('38.95142','-76.77055','01/22/2020','01/24/2020'))\n",
    "\n",
    "#for line in Flatten_Data(Get_Locations('Data\\Site_Data.csv','01/22/2020','01/24/2020')):\n",
    "    #print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to S3 and Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Connections():\n",
    "    print(\"Starting\")\n",
    "    ctx = snowflake.connector.connect(\n",
    "        user='KWR1',\n",
    "        password='dataflow1!',\n",
    "        account='vistra.us-east-1.snowflakecomputing.com'\n",
    "        )\n",
    "    cs = ctx.cursor()\n",
    "    try:\n",
    "        cs.execute(\"USE DATABASE wholesale_dev;\")\n",
    "        one_row = cs.fetchone()\n",
    "        print(one_row[0])\n",
    "        \n",
    "    finally:\n",
    "        cs.close()\n",
    "    ctx.close()\n",
    "        \n",
    "#Connections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Vistra Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "conn_a2 = pyodbc.connect('driver={SQL Server};Database=A2ISO;uid=svc_wdl_adapt2_pre-prod;pwd=Vistra567!;server=application.dev.dynegy.adapt2iso.com')\n",
    "a2_cursor = conn_a2.cursor()\n",
    "\n",
    "## Connection to NOOS not currently working\n",
    "#pyodbc.drivers()\n",
    "#connectString = pyodbc.connect('driver={ODBC Driver 17 for SQL Server};Database=NOOSTST3;uid=ARM8;pwd=N3ver#shgare;server=lmeae1nporalo01.tceh.net:1530')\n",
    "\n",
    "\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for Tables and Return a list of all values in one specific attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RefList(C,S,T,i):\n",
    "    RL = []\n",
    "    try:\n",
    "        C.execute('SELECT * from '+S+'.'+T)\n",
    "        data = list(C.fetchall())\n",
    "        RL = [item[i] for item in data]\n",
    "        return RL\n",
    "    except:\n",
    "        print(\"Error, could not execute query\")\n",
    "        \n",
    "def Search(C,S,T):\n",
    "    try:\n",
    "        C.execute('SELECT * from '+S+'.'+T)\n",
    "        print(T,\"Column Descriptions:\")\n",
    "        desc = list(C.description)\n",
    "        x = 0\n",
    "        for d in desc:\n",
    "            print(x, d)\n",
    "            x += 1\n",
    "        print(\"\")\n",
    "        print(\"Data:\")\n",
    "        cname = [item[0] for item in desc]\n",
    "        print(cname)\n",
    "        row = cursor.fetchmany(50)\n",
    "        print(row)\n",
    "    except:\n",
    "        print(\"Error, could not execute query\")\n",
    "\n",
    "#RefList(a2_cursor, 'dbo', 'A2_AWARDS', 12)\n",
    "#Search(a2_cursor,'dbo', 'PJM_MSRS_ENERGY_MKT_CONG_LOSS_CH_SUB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Table Attributes for Define Datapoints Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PTA(C,S,Tables):\n",
    "    for T in Tables:\n",
    "        C.execute('SELECT * from '+S+'.'+T)\n",
    "        try:\n",
    "            row = list(C.description)\n",
    "            for item in row:\n",
    "                Type = 'VARCHAR'\n",
    "                Struc = 'X*'\n",
    "                if str(item[1]) == \"<class 'int'>\":\n",
    "                    Type = 'NUMBER'\n",
    "                    Struc = '#*'\n",
    "                elif str(item[1]) == \"<class 'decimal.Decimal'>\":\n",
    "                    Type = 'NUMBER'\n",
    "                    Struc = '#.#'\n",
    "                elif str(item[1]) == \"<class 'datetime.datetime'>\":\n",
    "                    Type = 'DATE'\n",
    "                    Struc = 'mm/dd/yyyy' \n",
    "                print([T,item[0],Type,Struc])\n",
    "\n",
    "    \n",
    "                          \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "#PTA(a2_cursor,'dbo',['PJM_MSRS_ENERGY_MKT_CONG_LOSS_CH_SUB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a Reference List and check that list against a database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadRef(f,i):\n",
    "    R = Load_CSV(f)\n",
    "    RList = [item[i] for item in R]\n",
    "    return RList\n",
    "\n",
    "\n",
    "def SearchForTables(C,S,Tables,RList):\n",
    "    TL = []\n",
    "    for T in Tables:\n",
    "        C.execute('SELECT * from '+S+'.'+T)\n",
    "        try:\n",
    "            row = list(cursor.description)\n",
    "            for item in row:\n",
    "                if item[0].upper() in RList:\n",
    "                    print([T,item[0]])\n",
    "\n",
    "        \n",
    "                          \n",
    "        except:\n",
    "            pass\n",
    "    return TL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Database Table for DDL Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepTableDDL(C,S,T):\n",
    "    C.execute('SELECT * from '+S+'.'+T)\n",
    "    Atts = []\n",
    "    try:\n",
    "        row = list(C.description)\n",
    "        for item in row:\n",
    "            Type = 'VARCHAR'\n",
    "            Struc = 'X*'\n",
    "            if str(item[1]) == \"<class 'int'>\":\n",
    "                Type = 'int'\n",
    "                Struc = '#*'\n",
    "            elif str(item[1]) == \"<class 'decimal.Decimal'>\":\n",
    "                Type = 'int'\n",
    "                Struc = '#.#'\n",
    "            elif str(item[1]) == \"<class 'datetime.datetime'>\":\n",
    "                Type = 'DATE'\n",
    "                Struc = 'mm/dd/yyyy' \n",
    "            Atts.append([T,item[0],Type])\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return Atts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Reference Tables and test the attributes in target files to see if the values are present in any reference attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Line(f,x):\n",
    "    Header = str(f[0][x])\n",
    "    l = [item[x] for item in f if item[x] != \"\"]\n",
    "    l.pop(0)\n",
    "    s = list(set(l))\n",
    "    return [Header,s]\n",
    "\n",
    "def Test_Line(l):\n",
    "    reflist = []\n",
    "    for f in glob.glob(r'Adapt2_DataPoints\\Reference\\*.csv'):\n",
    "        Ref = Load_CSV(f)\n",
    "        y = 0\n",
    "        while y < len(Ref):\n",
    "            try:\n",
    "                C = Get_Line(Ref,y)\n",
    "                Header = C[0]\n",
    "                n = 0\n",
    "                for a in l:\n",
    "                    if a in C[1]:\n",
    "                        n += 1\n",
    "                if n > 0:\n",
    "                    reflist.append([f,Header,n/len(l)])\n",
    "                else:\n",
    "                    pass\n",
    "            except:\n",
    "                pass\n",
    "            y += 1\n",
    "    return reflist\n",
    "                \n",
    "                \n",
    "def Find_Refs(f):\n",
    "    T = Load_CSV(f)\n",
    "\n",
    "    x = 0\n",
    "    while x < len(T[0]):\n",
    "        try:\n",
    "            L = Get_Line(T,x)\n",
    "            Header = L[0]\n",
    "            l = L[1]\n",
    "            D = Test_Line(l)\n",
    "            if D[2] != []:\n",
    "                print(Header,D)\n",
    "                print(\"\")\n",
    "        except:\n",
    "            pass\n",
    "        x += 1\n",
    "\n",
    "def Ref_Match():\n",
    "    for file in glob.glob('Adapt2_DataPoints\\*\\*.csv'):\n",
    "        print(file)\n",
    "        Find_Refs(file)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Snowflake DDL to get a list of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_Tables(S):\n",
    "    masterlist = []\n",
    "    table = []\n",
    "    for line in S:\n",
    "        if 'CREATE OR REPLACE TABLE' in line:\n",
    "            masterlist.append(table)\n",
    "            table = []\n",
    "            table.append(line)\n",
    "        else:\n",
    "            table.append(line)\n",
    "    masterlist.append(table)\n",
    "    masterlist.pop(0)\n",
    "    for item in masterlist:\n",
    "        for line in item:\n",
    "            line = Replacy(' ',line)\n",
    "            line = Replacy('(',line)\n",
    "            line = Replacy(')',line)\n",
    "            line = Replacy(',',line)\n",
    "            line = Replacy('\\t',line)\n",
    "            line = Replacy('\\n',line)\n",
    "            line = line.split(\"|\")\n",
    "            while '' in line:\n",
    "                line.remove('')\n",
    "            if line == []:\n",
    "                pass\n",
    "            else:\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse a JSON File Recursively and return a list of available attribute layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parse_Layer(l,n):\n",
    "    kvl = [[key, value,n] for key, value in l.items()]\n",
    "    return kvl\n",
    "\n",
    "def Rec_PL(f):\n",
    "    Layer_List = []\n",
    "    try:\n",
    "        Layer_List.append(Parse_Layer(Load_JSON(f),str(f)))\n",
    "    except:\n",
    "        Layer_List.append(Parse_Layer(Load_JSON(f)[0],str(f)))\n",
    "    for x in Layer_List:\n",
    "        for i in x:\n",
    "            if type(i[1]) == list:\n",
    "                a = 0\n",
    "                while a < len(i[1]):\n",
    "                    try:\n",
    "                        Layer_List.append(Parse_Layer(i[1][a],i[0]))\n",
    "                        #a += 1\n",
    "                    except:\n",
    "                        #print(a, 'failed')\n",
    "                        #a += 1\n",
    "                        continue\n",
    "                    a += 1\n",
    "    masterlist = []\n",
    "    for d in Layer_List:\n",
    "        typelist = []\n",
    "        for item in d:\n",
    "            typelist.append([item[2],item[0],str(type(item[1])).split(\"'\")[1]])\n",
    "        if typelist not in masterlist:\n",
    "            masterlist.append(typelist)\n",
    "            \n",
    "    headers = []\n",
    "    for item in masterlist:\n",
    "        for h in item:\n",
    "            if h[0] not in headers:\n",
    "                headers.append(h[0])\n",
    "    cleanlist = []\n",
    "    for h in headers:\n",
    "        uh = []\n",
    "        for item in masterlist:\n",
    "            for entry in item:\n",
    "                if entry[0] == h:\n",
    "                    if entry not in uh:\n",
    "                        uh.append(entry)\n",
    "        cleanlist.append(uh)\n",
    "    u_entries_full = []\n",
    "    for item in cleanlist:\n",
    "        name = item[0][0]\n",
    "        u_entries = []\n",
    "        entries = list(set([entry[1] for entry in item]))\n",
    "        for e in entries:\n",
    "            types = [entry[2] for entry in item if entry[1] == e]\n",
    "            u_entries.append([name,e,types])\n",
    "        u_entries_full.append(u_entries)\n",
    "    return u_entries_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for printing tables and DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Table_DDL(l,source):\n",
    "    Directory = str('Create_Table_DDL\\\\'+source.upper()+\"_\"+\"WDL_DDL.SQL\")\n",
    "    with open(Directory, 'w') as export:\n",
    "        export.write('USE DATABASE wholesale_dev;')\n",
    "        export.write('\\n')\n",
    "        export.write('USE SCHEMA master_confidential;')\n",
    "        export.write('\\n')\n",
    "        for item in l:\n",
    "            tablename = str(source.upper()+'_'+item[0][0].upper())\n",
    "\n",
    "            export.write('CREATE OR REPLACE TABLE '+tablename+'_STG (')\n",
    "            export.write(\"\\n\")\n",
    "            for entry in item:\n",
    "                try:\n",
    "                    if entry[1].upper() == \"SYSTEM_REFERENCE\" or entry[1].upper() == \"SORT_ORDER\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        if 'int' in entry[2] or 'float' in entry[2] or 'NUMBER' in entry[2]:\n",
    "                            export.write(entry[1].upper()+\" NUMBER(38,2)\")\n",
    "                        elif 'bool' in entry[2] or 'BOOLEAN' in entry[2]:\n",
    "                            export.write(entry[1].upper()+\" BOOLEAN\")\n",
    "                        elif 'date' in entry[2] or 'TIMESTAMP' in entry[2]:\n",
    "                            export.write(entry[1].upper()+\" TIMESTAMP_LTZ\")\n",
    "                        else:\n",
    "                            export.write(entry[1].upper()+\" VARCHAR(50)\")\n",
    "                        export.write(\",\")\n",
    "                        export.write(\"\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "            export.write(\"DWH_CREATION_TIME TIMESTAMP_NTZ,\")\n",
    "            export.write(\"\\n\")\n",
    "            export.write(\"DWH_CREATION_USER  VARCHAR(50),\")\n",
    "            export.write(\"\\n\")\n",
    "            export.write(\"DWH_UPDATED_TIME TIMESTAMP_NTZ,\")\n",
    "            export.write(\"\\n\")\n",
    "            export.write(\"DWH_UPDATED_USER  VARCHAR(50),\")\n",
    "            export.write(\"\\n\")\n",
    "            export.write(\"SOURCE_SYSTEM_KEY  NUMBER(38,2)\")\n",
    "            export.write(\"\\n\")\n",
    "            export.write(\");\")\n",
    "            export.write(\"\\n\")\n",
    "            export.write(\"\\n\")\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Parsing Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackPac_Settled_NaturalGas : 9355\n",
      "BackPac_Forward_ERCOT_Hourly_Other : 5358662\n",
      "BackPac_WACOG_Coal : 0\n",
      "BackPac_Settled_ERCOT2 : 0\n",
      "BackPac_Forward_Emission_REC_Weather : 114792\n",
      "BackPac_Forward_ERCOT_Hourly_LZ : 2622139\n",
      "BackPac_Forward_ERCOT_Hourly_CoalPlant : 2116992\n",
      "BackPac_Forward_Vol_Coal_Oil : 1926\n",
      "BackPac_Settled_Oil : 383\n",
      "BackPac_Settled_ExERCOT : 1429395\n",
      "BackPac_Forward_ERCOT_Hourly_Hub : 3289914\n",
      "BackPac_Settled_NaturalGas2 : 0\n",
      "BackPac_Forward_PJM_Hourly : 2742070\n",
      "BackPac_BPE_AS_Factors2 : 0\n",
      "BackPac_Forward_ERCOT_Hourly_GasPlant : 3508767\n",
      "BackPac_Forward_ERCOT : 123584\n",
      "BackPac_Forward_InterestRate : 282880\n",
      "BackPac_Settled_Oil2 : 0\n",
      "BackPac_Forward_Coal : 33936\n",
      "BackPac_BPE_AS_Factors : 90048\n",
      "BackPac_Settled_ERCOT : 821583\n",
      "BackPac_Forward_Oil_Nuclear : 7079\n",
      "BackPac_CRR_Basis : 234\n",
      "BackPac_BPE_Power_Factors : 387840\n",
      "BackPac_Forward_NaturalGas_Options : 33637\n",
      "BackPac_Forward_Vol_NaturalGas_Power : 152792\n",
      "BackPac_Forward_ExERCOT : 34188\n",
      "BackPac_Forward_NaturalGas : 44976\n",
      "BackPac_Forward_NY_NE_CA_MISO_Hourly : 2077777\n"
     ]
    }
   ],
   "source": [
    "def ParqParse(f):\n",
    "    pf = ParquetFile(f)\n",
    "    df = pf.to_pandas()\n",
    "    fpath = str(f).split(\".\")[0]\n",
    "    tablename = str(fpath.split(\"\\\\\")[-2])\n",
    "    table = []\n",
    "    for col, d in zip(df.columns, df.dtypes):\n",
    "        att = [tablename, col, str(d)]\n",
    "        table.append(att)\n",
    "    return table\n",
    "\n",
    "\n",
    "def GetParqAtt(f):\n",
    "    pf = ParquetFile(f)\n",
    "    df = pf.to_pandas()\n",
    "    fpath = str(f).split(\".\")[0]\n",
    "    tablename = str(fpath.split(\"\\\\\")[-2])\n",
    "    table = []\n",
    "    print(df.iloc[0])\n",
    "    for col, d in zip(df.columns, df.dtypes):\n",
    "        print([tablename, col, str(d)])\n",
    "        \n",
    "\n",
    "def Convert_Parq():  \n",
    "    pass\n",
    "    #for file in glob.glob(r'C:\\Users\\kwrs\\Desktop\\TalendDemo\\pricing\\backpack\\test3\\dbo\\Stg_Mstar_List_Req_Data\\*'):\n",
    "        #pf = ParquetFile(file)\n",
    "        #df = pf.to_pandas()\n",
    "        #fpath = str(file).split(\".\")[0]\n",
    "        #tablename = str(fpath.split(\"\\\\\")[-1])\n",
    "        #source = str(fpath.split(\"\\\\\")[-2])\n",
    "    \n",
    "        #df.to_csv(r'C:\\Users\\kwrs\\Desktop\\TalendDemo\\BACKPAC\\NewStaging\\\\'+source+tablename+'.csv',index=False, sep = '|',quoting=csv.QUOTE_NONNUMERIC)\n",
    "    \n",
    "    #f = r'C:\\Users\\kwrs\\Desktop\\TalendDemo\\pricing\\backpack\\test3\\dbo\\Transform_Config_Data\\LOAD00000001.parquet'\n",
    "    #pf = ParquetFile(f)\n",
    "    #df = pf.to_pandas()\n",
    "    #fpath = str(f).split(\".\")[0]\n",
    "    #tablename = str(fpath.split(\"\\\\\")[-2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #PriceIndex = list(set(df['PriceIndex'].tolist()))\n",
    "    #for line in PriceIndex:\n",
    "        #if '<year>' in line:\n",
    "            #print(line)\n",
    "\n",
    "    #is_ind = df[\"StgListReqDataID\"]==1635911416\n",
    "    #print(df[is_ind])\n",
    "    #df.to_csv(r'C:\\Users\\kwrs\\Desktop\\TalendDemo\\pricing\\backpack\\\\'+tablename+'.csv',index=False, sep = '|',quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "def Load_Pipe_CSV(f):\n",
    "    with open(f, 'r') as r:\n",
    "        S = [item.split(\"|\")for item in r.readlines()]\n",
    "    return S\n",
    "\n",
    "def Index_Counter():\n",
    "    H = Load_Pipe_CSV(r'C:\\Users\\kwrs\\Desktop\\TalendDemo\\BACKPAC\\StageTest\\Header\\Stg_Mstar_List_Req_Hdr.csv')\n",
    "\n",
    "    T = Load_Pipe_CSV(r'C:\\Users\\kwrs\\Desktop\\TalendDemo\\BACKPAC\\StageTest\\Transform\\Transform_Config_Data.csv')\n",
    "\n",
    "    PI = list(set([item[6] for item in T]))\n",
    "\n",
    "    LN = list(set([item[4] for item in T]))\n",
    "\n",
    "    \n",
    "    Index = list(set([(item[0],item[1]) for item in H]))\n",
    "    cnt = Counter()\n",
    "    for file in glob.glob(r'C:\\Users\\kwrs\\Desktop\\TalendDemo\\BACKPAC\\StageTest\\Data\\*'):\n",
    "        \n",
    "        D = Load_Pipe_CSV(file)\n",
    "        D.pop(0)\n",
    "        HC = [item[1] for item in D]\n",
    "        for h in HC:\n",
    "            cnt[h] += 1\n",
    "    #print(cnt)\n",
    "    with open('Data\\Counter', 'wb') as WR:\n",
    "        pickle.dump(cnt,WR)\n",
    "    \n",
    "#Index_Counter()\n",
    "\n",
    "with open('Data\\Counter', 'rb') as RR:\n",
    "    cnt = pickle.load(RR)\n",
    "    \n",
    "#print(type(cnt))\n",
    "\n",
    "#print(cnt.values())\n",
    "\n",
    "H = Load_Pipe_CSV(r'C:\\Users\\kwrs\\Desktop\\TalendDemo\\BACKPAC\\StageTest\\Header\\Stg_Mstar_List_Req_Hdr.csv')\n",
    "HH = H.pop(0)\n",
    "#print(HH)\n",
    "Index = [[item[1],item[0]] for item in H]\n",
    "LN = list(set([item[0] for item in Index]))\n",
    "IndexMaster = []\n",
    "for l in LN:\n",
    "    Hlist = [l,[item[1] for item in Index if item[0] == l]]\n",
    "    IndexMaster.append(Hlist)\n",
    "    \n",
    "for i in IndexMaster:\n",
    "    rows = []\n",
    "    for c,v in zip(cnt.keys(),cnt.values()):\n",
    "        if c in i[1]:\n",
    "            rows.append(v)\n",
    "    i.append(sum(rows))\n",
    "for i in IndexMaster:\n",
    "    print(i[0],\":\",i[2])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Excel Mapping Spec for DDL Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepExcel(f,X):\n",
    "\n",
    "    x1 = pd.ExcelFile(f)\n",
    "    Admin_sheets = ['Change Log', 'BlankDim', 'BlankFact','TEMPLATE']\n",
    "    sheets = [name for name in x1.sheet_names if name not in Admin_sheets]\n",
    "    FullDS = []\n",
    "    for s in sheets:\n",
    "        dataset = []\n",
    "        att = []\n",
    "        dfn = x1.parse(s)\n",
    "        try:\n",
    "            Table_Name = str(dfn.columns[1].replace(X,''))\n",
    "            Table_Name = Table_Name.replace('_STG','')\n",
    "\n",
    "            df = x1.parse(s,header=9)\n",
    "            for row in df.iterrows():\n",
    "                index, data = row\n",
    "                dataset.append(data.tolist())\n",
    "            for line in dataset:\n",
    "                try:\n",
    "                    if 'DWH_CREATION_TIME' in line or 'DWH_CREATION_USER' in line:\n",
    "                        pass\n",
    "                    else:\n",
    "                    \n",
    "                        att.append([Table_Name,line[0],line[3]])\n",
    "                except:\n",
    "                    pass\n",
    "            FullDS.append(att)\n",
    "        except:\n",
    "            pass\n",
    "    return FullDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Talend-Readable XMLs from Excel Mapping Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_XMLs(f):\n",
    "    \n",
    "    TMDMAP = list(csv.reader(open(r'Talend_Metadata_Mapping.csv', 'rt'), delimiter=','))\n",
    "    TMDMAP.pop(0)\n",
    "\n",
    "    SFMD = list(set([item[2] for item in TMDMAP if item[2] != 'For Reference']))\n",
    "    errorlog = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #dataset = []\n",
    "    x1 = pd.ExcelFile(f)\n",
    "    Admin_sheets = ['Change Log', 'BlankDim', 'BlankFact']\n",
    "    sheets = [name for name in x1.sheet_names if name not in Admin_sheets]\n",
    "    for s in sheets:\n",
    "        dataset = []\n",
    "        dfn = x1.parse(s)\n",
    "        Table_Name = str(dfn.columns[1]+'.xml')\n",
    "        Directory = str('XML_Mapping_Files\\\\'+Table_Name)\n",
    "\n",
    "        df = x1.parse(s,header=9)\n",
    "        for row in df.iterrows():\n",
    "            index, data = row\n",
    "            dataset.append(data.tolist())\n",
    "        #dataset.pop(0)\n",
    "\n",
    "\n",
    "\n",
    "        with open(Directory, 'w') as export:\n",
    "            export.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?><schema>')\n",
    "    \n",
    "\n",
    "\n",
    "            for line in dataset:\n",
    "                if str(line[0]) == 'nan':\n",
    "                    name = \"\"\n",
    "                else:\n",
    "                    name = line[0]\n",
    "                if str(line[2]) == 'nan':\n",
    "                    comment = \"\"\n",
    "                else:\n",
    "                    comment = line[2]\n",
    "                datatype = \"\"\n",
    "                originalDbColumnName = \"\"\n",
    "                pattern = \"\"\n",
    "                try:\n",
    "                    if line[3] not in SFMD:\n",
    "                        errorlog.append([Table_Name,line])\n",
    "                        continue\n",
    "                except:\n",
    "                    continue\n",
    "                else:\n",
    "                    for entry in TMDMAP:\n",
    "                        if entry[2] == line[3]:\n",
    "                            datatype = entry[1]\n",
    "                            originalDbColumnName = line[3]\n",
    "                if datatype == 'id_Date':\n",
    "                    pattern = '&quot;dd-MM-yyyy&quot;'\n",
    "                if str(line[4]) == 'nan':\n",
    "                    length = \"-1\"\n",
    "                else:\n",
    "                    length = str(line[4]).split('.')[0]\n",
    "                if str(line[5]) == 'nan':\n",
    "                    precision = \"-1\"\n",
    "                else:\n",
    "                    precision = str(line[5]).split('.')[0]\n",
    "                key = 'false'\n",
    "                if str(line[6]).upper() == 'Y':\n",
    "                    key = 'true'\n",
    "                nullable = 'true'\n",
    "                if str(line[8]).upper() == 'N':\n",
    "                    nullable = 'false'\n",
    "                export.write('<column comment=\"'+comment+'\" default=\"\" key=\"'+key+'\" label=\"'+name+'\" length=\"'+length+'\" nullable=\"'+nullable+'\" originalDbColumnName=\"\" originalLength=\"\" pattern=\"'+pattern+'\" precision=\"'+precision+'\" talendType=\"'+datatype+'\" type=\"\"/>')\n",
    "            export.write('</schema>')\n",
    "\n",
    "    for line in errorlog:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsDate(x):\n",
    "    try:\n",
    "        t = datetime.datetime(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def IsFloat(x):\n",
    "    try:\n",
    "        t = float(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "        \n",
    "    \n",
    "def ParseXML(f):\n",
    "    tree = ET.parse(f)\n",
    "    root = tree.getroot()\n",
    "    #Attributes\n",
    "    tl = list(set([item.tag for item in root.iter()]))\n",
    "    for t in tl:\n",
    "        #Data\n",
    "        slist = list(set([item.text for item in root.iter(t)]))\n",
    "        if t.split('}')[-1] == 'REPORT':\n",
    "            NAME = slist[0]\n",
    "    att_list = []\n",
    "    for t in tl:\n",
    "        slist = list(set([item.text for item in root.iter(t)]))\n",
    "        tname = t.split('}')[-1]\n",
    "        if slist != ['\\n']:\n",
    "            att = [NAME,tname]\n",
    "            if IsDate(slist[0]) == True:\n",
    "                att.append(\"TIMESTAMP\")\n",
    "            elif IsFloat(slist[0]) == True:\n",
    "                att.append(\"NUMBER\")\n",
    "            else:\n",
    "                att.append('STRING')\n",
    "            att_list.append(att)\n",
    "    return att_list\n",
    "\n",
    "\n",
    "def Bulk_Create_DDL():\n",
    "    L = []\n",
    "    for file in glob.glob(r'Data\\CAISO_MAG\\*'):\n",
    "        l = ParseXML(file)\n",
    "        if l not in L:\n",
    "            L.append(l)\n",
    "\n",
    "    Create_Table_DDL(L,'CAISO_MA')\n",
    "    \n",
    "    \n",
    "def Create_ATT_List():\n",
    "    L = []\n",
    "    for file in glob.glob(r'Data\\CAISO_MAG\\*'):\n",
    "        l = ParseXML(file)\n",
    "        for line in l:\n",
    "            print(file,line)\n",
    "        #if l not in L:\n",
    "            #L.append(l)\n",
    "\n",
    "    #Create_Table_DDL(L,'CAISO_MA')\n",
    "    \n",
    "#Create_ATT_List()\n",
    "    \n",
    "\n",
    "Create_Table_DDL(PrepExcel(r'Mapping_Spec\\Adapt2 Mapping Spec.xlsm','ADAPT2_'),'ADAPT2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L = Load_CSV('Data\\MISO_AO_DETERMINANTS_HOURLY_202002190913.csv')\n",
    "#print(len(L))\n",
    "\n",
    "#for line in L:\n",
    "    #print([line[0],line[1].split(\" at \")[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Wiki(w):\n",
    "    raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/'+w)\n",
    "    raw_html = raw_html.read()\n",
    "\n",
    "    article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
    "    article_paragraphs = article_html.find_all('p')\n",
    "    article_text = ''\n",
    "\n",
    "    for para in article_paragraphs:\n",
    "        article_text += para.text\n",
    "\n",
    "    article_text = article_text.lower()\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP-Based Fuzzy Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NG_Char(t, chars):\n",
    "    ngrams = []\n",
    "    for i in range(len(t)-chars+1):\n",
    "        seq = t[i:i+chars]\n",
    "        ngrams.append(seq)\n",
    "    return ngrams\n",
    "\n",
    "def NG_Char_Compare(s,t,chars):\n",
    "    ng1 = NG_Char(s,chars)\n",
    "    ng2 = NG_Char(t,chars)\n",
    "    score = 0\n",
    "    for k in ng1:\n",
    "        if k in ng2:\n",
    "            score += 1\n",
    "    return score\n",
    "    \n",
    "def Predictor(s,tlist,chars):\n",
    "    score = 0\n",
    "    for t in tlist:\n",
    "        val = NG_Char_Compare(s,t,chars)\n",
    "        if val > score:\n",
    "            score = val\n",
    "            word = t\n",
    "    return word\n",
    "    \n",
    "def NG_Char_Predictor(ngrams):\n",
    "    curr_sequence = t[0:chars]\n",
    "    output = curr_sequence\n",
    "    for i in range(200):\n",
    "        if curr_sequence not in ngrams.keys():\n",
    "            break\n",
    "        possible_chars = ngrams[curr_sequence]\n",
    "        next_char = possible_chars[random.randrange(len(possible_chars))]\n",
    "        output += next_char\n",
    "        curr_sequence = output[len(output)-chars:len(output)]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def NG_Word(words):\n",
    "    ngrams = {}\n",
    "\n",
    "\n",
    "    words_tokens = nltk.word_tokenize(article_text)\n",
    "    for i in range(len(words_tokens)-words):\n",
    "        seq = ' '.join(words_tokens[i:i+words])\n",
    "        print(seq)\n",
    "        if  seq not in ngrams.keys():\n",
    "            ngrams[seq] = []\n",
    "        ngrams[seq].append(words_tokens[i+words])\n",
    "    \n",
    "    curr_sequence = ' '.join(words_tokens[0:words])\n",
    "    output = curr_sequence\n",
    "    for i in range(50):\n",
    "        if curr_sequence not in ngrams.keys():\n",
    "            break\n",
    "        possible_words = ngrams[curr_sequence]\n",
    "        next_word = possible_words[random.randrange(len(possible_words))]\n",
    "        output += ' ' + next_word\n",
    "        seq_words = nltk.word_tokenize(output)\n",
    "        curr_sequence = ' '.join(seq_words[len(seq_words)-words:len(seq_words)])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footer to Make sure that a Run-All-Cells Completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successly Ran All Cells\n"
     ]
    }
   ],
   "source": [
    "print(\"Successly Ran All Cells\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
